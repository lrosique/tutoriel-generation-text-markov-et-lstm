{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM et génération de texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce tutoriel présente l'utilisation d'un LSTM (un type particulier de réseau de neurones récurrents RNN) pour générer du texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif étant de générer des mots (et non des caractères, comme on retrouve parfois dans la littérature), on va découper notre texte grâce à une expression régulière pour récupérer tous les mots. \n",
    "\n",
    "On met également le texte en minuscule car les majuscules n'apportent (quasiment) aucune information. \n",
    "\n",
    "On pourrait également enlever les apostrophes ou autres signes de ponctuation pour améliorer le résultat final, et certains mots de liaisons/nombres..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import du texte\n",
    "file_name = \"la-machine-a-explorer-le-temps--h-g-wells.txt\"\n",
    "with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "words = re.findall(r\"[\\w]+\", text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping entre les mots et des clés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les mots n'étant pas compréhensibles par le réseau de neurones, on les converti en nombres pour qu'il puisse travailler efficacement avec (moins de mémoire est nécessaire). On enlève également les doublons, car ils n'ont aucun intérêt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Construction du mapping mot <-> nombre\n",
    "#On trie par ordre alphabétique notre liste de mots en enlevant les doublons\n",
    "reduced_words = list(sorted(set(words)))\n",
    "#Puis on associe une clé à chaque mot (qui est sa position dans la liste)\n",
    "mapping_words = {word: index for index, word in enumerate(reduced_words)}\n",
    "#Nombre de mots différents dans le texte\n",
    "nb_of_words = len(reduced_words)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction des données d'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite on découpe notre texte en données d'apprentissage, qui sont des couples (\"N mots précédents\", \"mot à prédire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Construction des données d'apprentissage\n",
    "memory = 2\n",
    "previous_words = []\n",
    "next_word = []\n",
    "for i in range(0, len(words) - memory):\n",
    "    #On ajoute les mots qui précendent l'index\n",
    "    previous_words.append(words[i: i + memory])\n",
    "    #Et dans une autre liste on stock la prédiction associée\n",
    "    next_word.append(words[i + memory])\n",
    "\n",
    "nb_learning_data = len(previous_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On construit alors deux vecteurs qui regroupent toutes ces données d'apprentissage (X pour la première partie du couple et y pour la seconde). \n",
    "\n",
    "Les vecteurs sont des vecteurs numpy, qui seront donnés à Keras pour entraîner le LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On met le tout dans des vecteurs qui seront donnés à Keras\n",
    "#Remarque : il faut de la mémoire sur votre ordinateur\n",
    "#On initialise les vecteurs\n",
    "X = np.zeros((nb_learning_data, memory, nb_of_words), dtype=np.bool)\n",
    "y = np.zeros((nb_learning_data, nb_of_words), dtype=np.bool)\n",
    "#On alimente les vecteurs\n",
    "for i, sentence in enumerate(previous_words):\n",
    "    for t, word in enumerate(sentence):\n",
    "        X[i, t, mapping_words[word]] = 1\n",
    "    y[i, mapping_words[next_word[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Déclaration du modèle Keras de réseau de neurones récurrents (de type LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va utiliser un LSTM pour faire de la génération de texte. Un certain nombre de paramètres doit être précisé dont :\n",
    "- la taille du réseau\n",
    "- le nombre de couples que l'on va traiter en même temps pour entraîner le réseau (généralement, on ne fait pas \"un couple\" = \"une mise à jour du réseau pour l'améliorer\", car il oscille trop)\n",
    "- le nombre de fois que l'on va répéter la procédure d'apprentissage (idéalement 30 fois, si vous avez le temps)\n",
    "- la vitesse à laquelle le réseau va apprendre (si vous mettez un nombre trop élevé, il pourrait ne jamais atteindre la meilleure prédiction)\n",
    "\n",
    "Ensuite, on déclarera grâce à Keras notre réseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Modèle KERAS : un RNN particulier, le LSTM (long short term memory)\n",
    "# Paramètres\n",
    "rnn_size = 128\n",
    "batch_size = 30\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle\n",
    "model = Sequential()\n",
    "model.add(LSTM(rnn_size, input_shape=(memory, nb_of_words)))\n",
    "model.add(Dense(nb_of_words))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entraînement de l'algorithme\n",
    "model.fit(X, y,batch_size=batch_size,epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sauvegarde du LSTM entraîné\n",
    "model.save('lstm_text_generation.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédictions de l'algorithme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'image de ce qui a été fait avec Markov, on va prédire les mots suivants du début de notre phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prédictions\n",
    "#Variables\n",
    "start_sentence = \"L'intelligence artificielle\"\n",
    "nb_predictions = 10\n",
    "prediction = start_sentence\n",
    "#Initialisation de la mémoire\n",
    "last_words = re.findall(r\"[\\w]+\", start_sentence.lower())\n",
    "last_words = last_words[-memory:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On répète ce qui a été fait en termes de préparation des données, puis on génère le mot suivant de la phrase. Puisque notre algorithme a appris à prédire des nombres, il faut convertir son résultat en \"mot\" grâce au dictionnaire de mapping construit précédemment.\n",
    "\n",
    "Ensuite on met à jour la mémoire, pour prédire le mot d'après."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On boucle sur le nombre de prédictions à faire pour répéter la procédure indiquée\n",
    "for i in range(nb_predictions):\n",
    "    #Construction du vecteur d'entrée du modèle Keras\n",
    "    x = np.zeros((1, memory, nb_of_words))\n",
    "    for t, word in enumerate(last_words):\n",
    "        #Les mots sont convertis en nombres\n",
    "        x[0, t, mapping_words[word]] = 1.\n",
    "    #Prédictions du modèle : on a la probabilité de chaque mot du dictionnaire d'être le suivant\n",
    "    possibilities = model.predict(x, verbose=0)[0]  \n",
    "    #On prend le mot avec la probabilité la plus élevée\n",
    "    #En général, on préfère utiliser une notion de \"température\" pour laisser une part au hasard\n",
    "    key_word = np.argmax(possibilities)\n",
    "    #On traduit le nombre obtenu en mot grâce au dictionnaire\n",
    "    #Pour comprendre le code : on converti les clés du dictionnaire en une liste et pareil pour les valeurs\n",
    "    #Ensuite, on prend dans le dictionnaire des clés l'entrée qui a le même numéro que la valeur voulue\n",
    "    #Car rappelons-le, notre dictionnaire est de la forme \"mot\":nombre et nous on a un nombre là\n",
    "    pred = list(mapping_words.keys())[list(mapping_words.values()).index(key_word)]\n",
    "    \n",
    "    #On ajoute le mot prédit à la phrase\n",
    "    prediction += \" \"+pred\n",
    "    #Et on met à jour la mémoire\n",
    "    last_words.pop(0)\n",
    "    last_words.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche le résultat final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
